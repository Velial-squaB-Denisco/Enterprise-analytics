<div>
<img src="https://www.syssoft.ru/upload/iblock/5be/po7970wzyp58ws9j9ikmgiznih9m0sz2.png"/>
</div>

## 1.Наполняем core-уровень КХД 
---

### Краткое изложение

Статья описывает процесс наполнения Core-уровня корпоративного хранилища данных (КХД) очищенными данными. КХД состоит из нескольких уровней: Stage (опционально), ETL-процессы, Core-уровень и Data Mart. Основное внимание уделяется Core-уровню, где хранятся данные, полученные в результате ETL-процессов.

#### Основные моменты:

1. **Уровни КХД**:
   - **Stage**: Хранение данных без трансформаций.
   - **ETL-процессы**: Трансформация данных для удобного сбора витрин/моделей.
   - **Core-уровень**: Хранение данных для дальнейшего использования.
   - **Data Mart**: Формирование витрин и моделей для BI и других потребителей.

2. **Совместимость данных**:
   - Данные на Core-уровне должны быть совместимы друг с другом, как кубики конструктора, чтобы сохранялась единая логика связей и можно было легко изменять модели данных.

3. **Структура таблиц**:
   - Таблицы делятся на таблицы показателей (факты) и справочники.
   - Таблицы показателей включают поля для расчетов и даты, а справочники — поля для измерений.

4. **Процесс выгрузки данных**:
   - Описан пример сценария выгрузки данных в Core-уровень КХД с использованием LGP-файла.
   - Важно соблюдать правила нейминга полей и форматов дат для оптимизации будущих моделей/витрин.

5. **Рекомендации**:
   - Расчетные показатели лучше брать из источника, если это возможно.
   - Чем больше показателей посчитано на уровне Core, тем проще их визуализировать.
   - Расчеты, требующие высокой интерактивности, лучше делать на стороне BI.

6. **Сохранение данных**:
   - Результат ETL-процесса сохраняется в Clickhouse.
   - Используются компоненты для аудита таблиц и создания/адаптации таблиц в Clickhouse.

**Полезные моменты:**

- Подробное описание уровней КХД и их ролей.
- Практические рекомендации по подготовке данных для Core-уровня.
- Описание процесса выгрузки данных и использования компонентов для аудита и создания таблиц.

**Менее полезные моменты:**

- Некоторые технические детали могут быть сложны для понимания без контекста или опыта работы с КХД.
- Описание процесса выгрузки данных может быть избыточным для пользователей, не знакомых с конкретными инструментами (например, Loginom).

--- 
## Материал
---

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/day-2.png"/>
</div>

Разберем выгрузку очищенных данных на Core-уровень КХД. Сюда сохраняются данные, которые потом используются для построения витрин и моделей на уровне Data Mart.

КХД состоит из нескольких условных уровней:

- Уровень Stage (опционально) — выгрузка данных из источника как есть — без трансформаций, чтобы дальнейшие процессы не дергали источники.

- ETL-процессы — трансформации данных со Stage уровня и из источников, чтобы получить таблицы, из которых удобно собирать витрины/модели

- Core уровень — хранение данных, полученных в ходе ETL-процессов.

- Datamart уровень — тут формируются витрины и модели из таблиц Core уровня, к которым подключаются BI и другие потребители.
<div>
<img src="https://loginom.ru/sites/default/files/global-files/textpage/2025/data-warehouse_2.svg"/>
</div>

---

<h2 align="center">Аналитическое хранилище</h2>

Как вы знаете, Loginom предоставляет большое количество возможностей по трансформации, очистке и обогащению данных («Генеральная уборка данных»). Однако, чтобы эти данные стали частью КХД, недостаточно просто взять и выгрузить их в базу. Нам нужно, чтобы таблицы на Core-уровне были совместимы друг с другом, буквально как кубики конструктора или части мозаики.

Такая «квадратная» совместимость важна для того, чтобы при росте количества таблиц сохранялась единая логика связей, и мы могли легко изменять модели данных. Более того, только такой подход является реально масштабируемым и позволяет не закапываться в бесконечных исправлениях «уникально созданных» моделей.
<div>
<img src="https://loginom.ru/sites/default/files/global-files/textpage/2025/data-model-collection.svg"/>
</div>

<h2 align="center">Сбор модели данных</h2>

### С чем будем работать

Для наполнения КХД мы приготовили набор таблиц следующей структуры (в таком виде они будут лежать на Core-уровне, до организации в витрину или «Звезду»).

<div>
<img src="https://loginom.ru/sites/default/files/global-files/textpage/2025/table-structure.svg"/>
</div>

<h2 align="center">Сбор модели данных</h2>

В него входят:

- Таблицы показателей (таблицы фактов):
  - Продажи — в разрезе дней, менеджеров, клиентов и товаров;
  - План продаж — в разрезе месяцев, товаров и менеджеров;
  - Прогноз продаж — прогноз в разрезе месяцев и категорий товаров.
- Справочники:
  - Товары — справочник товаров;
  - Менеджеры продаж — сотрудники, закрепленные за продажами. По ним же строится и план продаж;
  - Клиенты — справочник клиентов:
  - Менеджеры клиентов — сотрудники, закрепленные за клиентами;
  - Классификаторы A и B — дополнительные справочники для усложнения модели.

Разделение на таблицы показателей и справочников условно. Таблица показателей обычно включает поля для расчетов (меры, measures; например, «Выручка» в таблице продаж) и даты, к которым привязаны эти данные. В ней также есть ключевые поля, которые связывают данные со справочными таблицами.

Справочники содержат поля, которые на стороне аналитических систем становятся измерениями (еще их называют разрезы, аналитики, dimensions). По значениям этих полей на визуализациях разбиваются результаты агрегаций. Например, в диаграмме, на которой показывается разбивка выручки по клиентам, поле «Клиент» является измерением.

При этом таблицы показателей могут содержать свои собственные поля измерений (когда эти поля не целесообразно выносить в отдельный справочник, т.к. они используются только для этой таблицы). А в условном справочнике клиентов может появиться поле «Дата создания», и вот уже мы можем посчитать показатель «Количество новых клиентов», посчитав идентификаторы клиентов в разрезе дат их создания.

В нашем подходе на уровне КХД таблицы справочников и полей ничем технически не отличаются. Это просто общепринятое обозначение для прояснения основной роли таблицы.

### Пример сценария для выгрузки данных в Core-уровень КХД

Откройте сценарий второго дня в папке M3\Data Monetization Pack\Examples. Скачайте пакет в папку.

<h2 align="center">
<a href="https://downloads.loginom.ru/day2-saving-data-core-level.lgp" style="background-color: #4CAF50; color: white; padding: 10px 20px; text-align: center; text-decoration: none; display: inline-block; border-radius: 5px;">Скачать файл LGP</a>
</h2>

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/file-lgp-1.png"/>
</div>

<h2 align="center">LGP-файл</h2>

Откройте пакет.

Для справки: когда вы создаете сценарий выгрузки данных в КХД, в вашем пакете должны находиться ссылки на 3 набора компонентов:
 - Clickhouse Kit — для взаимодействия с СУБД Clickhouse;
  - Data Preparation Kit — для быстрого просмотра состояния хранилища;
  - Data Control Kit — для аудита данных и оповещениях об инцидентах.

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/packet-2.png"/>
</div>

<h2 align="center">Пакет</h2>

В сценарии находится подмодель для очистки КХД. Можете запустить ее, если запутаетесь и захотите сбросить вашу базу к исходному состоянию, и проделать все операции с чистого листа.

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/cleaning-dwh-3.png"/>
</div>

<h2 align="center">Очистка КХД</h2>

Обычно в ETL-сценариях для КХД подготовка каждой таблицы происходит внутри ее подмодели. Зайдем в подмодель Sales plan, чтобы увидеть детали процесса.

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/submodel-sp-4.png"/>
</div>

<h2 align="center">Подмодель Sales plan</h2>

Импорт LGD-файла заменяет нам ETL-процесс, который происходит на этапах 1-2 схемы потока данных. В реальном сценарии тут скорее всего была бы подмодель с ETL-процессом.

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/replacing-submodel-5.png"/>
</div>

<h2 align="center">Замена подмодели с ETL-процессом</h2>

Разберем, как должна выглядеть правильно подготовленная таблица для core-уровня КХД.

## Как правильно подготовить данные для Core-уровня КХД

Откроем предпросмотр данных из LGD-файла плана продаж.

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/data-preview-6.png"/>
</div>

<h2 align="center">Предпросмотр данных</h2>

Технически, можно сохранить в БД любые данные. Но если мы хотим, чтобы в дальнейшем из них было легко собрать витрины/модели, в которые в будущем можно легко добавить другие таблицы, нужно соблюсти следующий набор правил.

## Нейминг полей — ключевые поля

Ключевые поля — это такие поля, по которым строится связь между таблицами, позволяя делать вычисления по полям одной таблицы в разрезе полей другой таблицы.

- Ключевые поля имеют свой префикс. В DMP (Data Monetization Pack) этот префикс определяется в файле настроек Data_prep.txt, по умолчанию key_;
- Ключевые поля называются одинаково во всех таблицах. Если идентификатор товара в справочнике товара называется key_Product_ID, он должен называться так во всех таблицах;
- Метка и имя ключевого поля должны быть одинаковыми.

Нейминг полей — поля данных:

- Не ключевое поле может иметь произвольную метку и имя;
- Для облегчения будущего сбора витрин, нужно заранее определить роли полей. Их может быть 3: мера, измерение, каноническая дата.

Меры определяются добавлением «_M» в конец метки поля. Каноническая дата определяется через добавление «_CD» в конец метки поля. Каноническая дата — термин из моделирования данных. Обозначает единую ось данных в модели/витрине, вокруг которой собираются показатели из разных таблиц, и их можно анализировать на одной временной оси (например, план и факт продаж). Одна таблица может содержать несколько канонических дат. Подробнее разберем эту тему позже.

Все поля, не являющиеся ключами, мерами или каноническими датами, становятся измерениями. Во время сборки витрины/модели, измерения могут быть связаны по ключевым полям с другими таблицами. Давая таким образом анализировать показатели одних таблиц в разрезе полей из других таблиц.

## Форматы дат:

- По возможности убирайте из полей дат значение времени (округляйте значение даты вниз до целого). Это оптимизирует работу будущих моделей/витрин;
- Если поле содержит не дату (например, день продаж), а обозначение периода (например, месяц плана) — такое значение должно быть приведено к первой дате периода (первый день месяца плана). Это позволит на одной временной оси связать данные разных разрядностей во времени и анализировать их совместно в более укрупненных группировках календаря (например, месяц, квартал, год).

## Общие рекомендации по полноте данных таблиц Core-уровня

Особенность потока превращения данных в аналитику в том, что преобразования данных могут происходить на каждом из этапов пути.

<div>
<img src="https://loginom.ru/sites/default/files/global-files/textpage/2025/system-nfe-1%20(1)_0.svg"/>
</div>

<h2 align="center">Поток данных</h2>

Можно посчитать все данные на стороне источника (особенно если это что-то вроде 1С), выгрузить это в BI и визуализировать простейшими формулами.

А можно взять самые кошмарные данные, загрузить их сразу в BI, написав там сложный ETL-процесс, трехэтажные формулы, и получить аналогичный результат.

А еще можно создавать новые аналитические признаки на этапе формирования модели/витрины. В общем, можно посчитать много что и много где. Как же в итоге лучше распределить на потоке преобразования вычисления, что и где лучше считать?

Вот некоторые рекомендации:

1. Если какой-то расчетный показатель может быть в готовом виде получен из источника — лучше взять его из источника (например, расчетные показатели из 1С вроде валовой прибыли);

2. Чем больше показателей будет посчитано на уровне Core, тем проще вам будет это визуализировать дальше, т.к. не придется полагаться на возможности выражений BI-системы. Ваши данные должны быть подготовлены на таком уровне, чтобы 90% показателей считалось максимум формулами вроде sum_if и ее аналогами.

Почему? Потому что чем больше специфических функций используется на стороне визуализации данных, тем менее универсальным является КХД, и тем больше вы привязаны к конкретному инструменту аналитики. Чем больше показателей и логики рассчитывается на core-уровне КХД, тем проще ее переиспользовать в любых инструментах. И к этому нам и нужно стремиться, но без фанатизма.

3. Расчеты, для которых требуется высокий уровень интерактивности, и которые нельзя заранее рассчитать на core-уровне, имеет смысл делать на стороне BI. Например, показатель «Средний чек», который считается как сумма продаж, деленная на количество продаж.

В аналитике этот показатель может требоваться выводить в самых разных разрезах с самыми разными фильтрами. Посчитать это на уровне таблицы данных заранее в таком количестве вариантов невозможно. Поэтому такие вещи смело считаем на стороне BI.

## Сохранение данных в core-уровень КХД

Результат ETL-процесса должен быть сохранены в Clickhouse, и за это отвечает компонент «📁 CH: Выгрузка в КХД».

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/replacing-submodel-5.png"/>
</div>

<h2 align="center">Замена подмодели с ETL-процессом</h2>

Этот компонент — часть библиотеки Clickhouse Kit.

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/clickhouse-kit.png"/>
</div>

<h2 align="center">Clickhouse Kit</h2>

Т.к. библиотеки из Data Monetization Pack содержат множество компонентов, которые, в том числе, должны комбинироваться друг с другом для решения рабочих задач, для упрощения работы используются готовые шаблоны сценариев (отмечены значком папки). Такие компоненты добавляются в сценарий как производный узел, т.е. подмодель в которую можно зайти, увидеть готовую схему связей компонентов, и донастроить ее под ваш сценарий.

Зайдем в подмодель «📁 CH: Выгрузка в КХД».

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/submodel-8.png"/>
</div>

<h2 align="center">Подмодель</h2>

Активируем компонент «Аудит таблицы». Он используется для проверки, все ли учтено для сохранения таблицы в core-уровень.

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/table-audit-9.png"/>
</div>

<h2 align="center">Компонент «Аудит таблицы</h2>

Первый порт пропускает входные данные без изменений.

Второй порт содержит метаданные, т.е. описание полей таблицы, которые будут использоваться для формирования интерактивной документации и автоматизаций КХД.

Часть метаданных формируется автоматически, а другая часть (в т.ч. произвольное описание полей) задается вами в настройках аудита (про них поговорим дальше).

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/metadata-10.png"/>
</div>

<h2 align="center">Метаданные</h2>

​​​​​Третий порт содержит результаты аудита. Если там есть строки с красным крестиком, значит не все было правильно подготовлено к сохранению в core-уровень. В данном случае, проблема в заполнении обязательных параметров.

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/audit-results-11.png"/>
</div>

<h2 align="center">Результаты аудита</h2>

Откроем второй порт переменных на входе в аудит.

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/table-audit-9%20%281%29.png"/>
</div>

<h2 align="center">Второй порт переменных</h2>

Заполните указанные параметры в соответствие с примером:

- Название таблицы — то, как будет называться таблица в базе core-уровня КХД. Рекомендуется писать английские буквы, без пробелов;
- Описание таблицы — краткий комментарий (произвольный текст) о том, что это за таблица;
- Первичный ключ — ключевое поле с префиксом key_, являющееся первичным ключом (если таковое есть в данных, иначе оставляем пустым. Пример с заполнением первичного ключа можно посмотреть в подмодели Products);
- Префикс имени/метки поля — аббревиатура, которая будет использована при построении витрин для идентификации полей, взятых из данной таблицы. Т.к. витрины соединяют в одной таблице поля из множества таблиц core-уровня, очень полезно понимать, из какой таблицы какое поле берется.

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/received-data-12.png"/>
</div>

<h2 align="center">Полученные данные</h2>

Сохраните настройки, запустите аудит повторно и откройте третий выходной порт. Вы должны увидеть отсутствие красных крестиков. Лампочка обозначает ситуацию, которая не является критической ошибкой, но влияет на то, как данные этой таблицы могут быть использованы в дальнейшем. Просто убедитесь, что пункты с лампочкой существуют в соответствие с вашей задумкой.

Проверьте также второй выходной порт — вы должны увидеть новые атрибуты в метаданных, которые появляются только при отсутствии проблем в аудите.

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/output-port-13.png"/>
</div>

<h2 align="center">Второй выходной порт</h2>

После успешного аудита может быть активирован узел сохранения метаданных. Если в аудите есть критические ошибки, сохранение не сработает, и дальнейший процесс выгрузки данных не будет произведен. Метаданные сохраняются в папке библиотеки DMP в виде LGD-файлов.

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/saving-node-14.png"/>
</div>

<h2 align="center">Узел сохранения метаданных</h2>

Чтобы данные могли быть выгружены в базу, в ней должна быть создана принимающая таблица. Компонент «CH: Создать / адаптировать MT таблицу» автоматически создает таблицу на основе полей данных.

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/receiving-table-15.png"/>
</div>

<h2 align="center">Принимающая таблица</h2>

Данный компонент является универсальным и может использоваться для создания таблиц в Clickhouse для произвольных задач. Разберем настройки этого компонента в порту переменных.

Прежде всего, здесь нужно указать базу данных и таблицу, которая будет принимать данные. Если их не существует — они будут созданы автоматически. Т.к. компонент используется в рамках преднастроенной схемы, в нем уже указана принимающая БД (временная база для core-уровня) и название таблицы (на основе значения, указанного в аудите).

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/data-16.png"/>
</div>

<h2 align="center">Данные</h2>

Далее, идет набор специфичных для Clickhouse настроек. Краткая справка по основам хранения данных в Clickhouse:

У каждой таблицы есть движок. Движок определяет способ хранения данных и логику отработки запросов. Основной движок, использующий преимущества СУБД Clickhouse — MergeTree() и его разновидности. Он установлен в настройках по умолчанию.

Фишка Clickhouse — в быстрых выборках больших объемов данных. Однако, чтобы этот функционал работал более оптимально, для таблицы можно задать ряд дополнительных настроек:

Поле Партиции. Партиция — это поле, по которому идет разбивка на большие блоки данных с целью их быстрой переброски между разными таблицами. Имеет смысл использовать при наличии в партициях от 100 000 строк. Раньше имели большую роль в оптимизации чтения, но теперь эта роль сместилась в параметр Order By, указываемый при создании таблицы.

Поля сортировки Order By — основной способ ускорения чтения данных из MergeTree таблиц. Здесь обычно указывают набор полей, по которым происходит группировка данных при запросах из таблицы. При этом поля указываются в порядке возрастания частотности значений (количество уникальных значений или кардинальность).

Обычно это важно для таблиц, к которым идут запросы вида Group by (здесь перечисляются наиболее часто используемые поля в Group by), что довольно редко происходит для таблиц core-уровня, т.к. они используются для сборок витрин в виде материализованных таблиц на базе select-запросов. А вот к витринам уже идут group by запросы из BI. Таким образом, без особой необходимости для таблиц core-уровня здесь ничего можно не писать.

Однако, этот параметр является обязательным для создания таблицы. Поэтому оставим его пустым, таблица будет создана с сортировкой order by Tuple(), что эквивалентно сортировке по всем полям в порядке их создания. Для более оптимальной работы нужно, чтобы поля были расставлены в порядке возрастания частотности значений.

Чтобы не заниматься расстановкой порядка полей вручную (определяется порядком на входном порту), можно активировать опцию «Порядок полей по частотности при создании таблицы».

Тогда перед созданием таблицы будет проведен анализ частотности значений (может занять некоторое время), и порядок определится автоматически.

Поля первичного ключа — это не тот первичный ключ, о котором вы подумали (и он не имеет отношения к первичному ключу, указанному в аудите). В Clickhouse есть разновидность MergeTree движков, которые умеют агрегировать данные в фоновом режиме. Делают они это в разрезе полей, указанных в Order By. Например, в таблицу пишется информация из чековой ленты, разбитая по товарам внутри чека (т.е. по несколько строк на 1 чек). Но за счет указания Order By вида Город, Магазин, Дата, ИД чека и использования движка SummingMergeTree, данные суммируются на уровне чека.

Однако, такой Order By не является оптимальным с точки зрения использования group by запросов, т.к. в нем содержится слишком детализированное поле ИД чека. И вот для этих случаев используется параметр PrimaryKey — в него можно дописать часть полей, которые уже используются в Order By. Например, только Город, Магазин, Дата. Это сделает работу group by запросов более оптимальной.

### В целом для сохранения данных в core-уровень КХД вы можете держать все эти настройки пустыми. Они нужны прежде всего для специальных таблиц-витрин, к которым обращаются аналитические инструменты.

Параметр Nullable_mode (0 — все nullable, 1 — все не nullable, 2 — автоопределение) отвечает за то, могут ли принимать поля таблицы значения NULL или нет. При создании таблицы, для каждого поля может быть определена эта опция. Nullable-поля работают менее оптимально по сравнению с не-nullable с точки зрения скорости выполнения запросов. Но при этом нет смысла не использовать их, если такова логика данных (например, отсутствующее значение в ключевых полях все-таки имеет смысл хранить как null, чтобы не получить искусственный пустой ключ, на который будут завязаны все несвязанные данные).

Режим «2 — автоопределение» задает атрибут Nullable/не Nullable при создании таблицы в зависимости от наличия null-значений в текущем наборе данных.

Удалить и пересоздать. Если принимающая таблица уже существует в БД, то будет проведена проверка соответствия сохраняемой таблицы из Loginom и принимающей таблицы. При обнаружении расхождений будет выполнен запрос ALTER TABLE для того, чтобы принимающая таблица соответствовала по структуре сохраняемой. Возможные изменения: добавление полей, удаление полей, смена типов данных в поле.

Иногда возможны ситуации, когда изменение таблицы через запрос ALTER TABLE невозможно (например, вы переименовали все поля в сохраняемой таблицы, и теперь для актуализации нужно удалить все поля в принимающей таблице и создать новые с новыми именами, или происходит конвертация несовместимых типов данных). Тогда этот компонент будет возвращать ошибку и останавливать процесс сохранения данных. Чтобы исправить ситуацию, таблицу нужно пересоздать с нуля. Для этого можно активировать данную опцию и выполнить компонент. Не забудьте потом отключить ее, чтобы таблица не пересоздавалась при каждой активации компонента.

После активации компонента, на выходе окажутся:

1. Ваши данные, которые проходят насквозь без изменений;

2. Выполнение запроса DESCRIBE TABLE к целевой таблице показывает, что она реально существует в БД.

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/execution-request-17.png"/>
</div>

<h2 align="center">Выполнение запроса</h2>

​​​​​3. Отчет о результатах активации компонента. Там в числе прочего можно увидеть действия, выполненные над таблицей и запрос (создание/изменение/ничего). А также шаблоны запросов Select с подстановкой алиасов и без — для быстрого использования в других местах.

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/report-18.png"/>
</div>

<h2 align="center">Отчет</h2>

Далее отрабатывает стандартный узел экспорта в БД. В нем можно настроить способ обновления данных (частичная / полная перезапись).

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/export-19.png"/>
</div>

<h2 align="center">Экспорт в БД</h2>

За счет того, что в сопоставлении полей включена автосинхронизация, вам не нужно настраивать его вручную. Кроме того, при изменении структуры таблицы изменение сопоставления полей будет происходить автоматически.

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/auto-sync-20.png"/>
</div>

<h2 align="center">Автосинхронизация</h2>

Финальный этап — публикация таблицы. Этот компонент мгновенно переносит таблицу из текущего положения в новое («Старая БД».«Старая таблица» > «Новая БД. Новая таблица»).

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/publication-table-21.png"/>
</div>

<h2 align="center">Публикация таблицы</h2>

В случае сценария выгрузки в КХД, это означает перенос из временного core-уровня в рабочий. Этот процесс необходим, чтобы во время длительных процессов записи данных не допустить чтения неполных данных из рабочей базы. Также у компонента есть опция — создать клон публикуемой таблицы в старом расположении. Таким образом, можно одновременно иметь 2 версии таблицы, тестовую и рабочую. При этом процесс публикации данных можно отключить в настройках компонента, чтобы данные обновлялись только в тестовой таблице.

Для эксперимента можете подать данные на выгрузку через параметры полей (данные подаются не из Sales plan, а через параметры полей. Набор полей меняется на входе в компонент и дальше автоматически меняется в БД, позволяя принять новую таблицу), и активировать выгрузку в КХД.

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/field-options-22.png"/>
</div>

<h2 align="center">Параметры полей</h2>

1. Таким образом убедитесь, что изменение таблицы не потребовало от вас перенастройки сценария;
2. «CH: Создать / адаптировать MT таблицу» на сообщение о том, что выполнен запрос на изменение принимающей таблицы.

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/publication-table-21.png"/>
</div>

<h2 align="center">Публикация таблицы</h2>

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/execution-request-23.png"/>
</div>

<h2 align="center">Выполнение запроса</h2>

### Переключите подачу данных на выгрузку обратно на LGD-файл и повторно выполните выгрузку.

Вернитесь на исходный сценарий и выполните всю цепочку обновления данных (там везде внутри все по аналогии с разобранным примером Sales plan).

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/data-update-24.png"/>
</div>

<h2 align="center">Обновление данных</h2>

### Проверяем результат работы
Добавьте в сценарий импорт из базы данных на основе готового подключения Clickhouse ADWH.

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/import-database-25.png"/>
</div>

<h2 align="center">Импорт из базы данных</h2>

В настройках узла импорта подтвердите наличие девяти таблиц во временной и рабочей core-базах.

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/node-setup-26.png"/>
</div>

<h2 align="center">Настройка узла</h2>

В производных компонентах, в разделе Data_Preparation_Kit, добавьте в сценарий компонент «3.1. Список таблиц (мета-справочник)».

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/adding-component-27.png"/>
</div>

## 2.Мониторинг никогда не спит
---

### Краткое изложение урока

Этот урок посвящен важной теме мониторинга качества данных. Основные моменты:

1. **Аудит качества данных**:
   - Использование компонента «Аудит качества данных» для проверки данных на наличие проблем.
   - Создание логических полей для контроля значений, например, проверка значений в поле `Amount_plan`.

2. **Производные узлы**:
   - Возможность настройки и добавления собственных узлов в производные узлы для улучшения проверки данных.
   - Пример добавления узла «Дубликаты и противоречия» для проверки дубликатов.

3. **Отчеты и уведомления**:
   - Формирование сводной информации о проблемах в данных с помощью компонента «Отчет DQ».
   - Интеграция с Telegram-ботом для оперативного уведомления о выявленных проблемах.

4. **Визуализация и автоматизация**:
   - Использование встроенных визуализаторов для быстрого анализа данных.
   - Автоматизация процесса проверки данных с помощью планировщика.

### Оповещения в Telegram

Интеграция с Telegram-ботом является важной частью урока, так как она позволяет оперативно получать уведомления о проблемах с данными без необходимости постоянно проверять систему вручную. Вот более детальная информация об этом аспекте:

1. **Настройка Telegram-бота**:
   - В уроке упоминается, что на первом занятии были созданы предварительные настройки для Telegram-бота. Этот бот используется для отправки сообщений в технический чат, когда в данных обнаруживаются проблемы.
   - Бот активируется последним компонентом в сценарии, что позволяет своевременно информировать ответственных лиц о необходимости вмешательства.

2. **Компонент «Отправить сообщение»**:
   - Для отправки уведомлений используется компонент «Отправить сообщение» из библиотеки API_Telegram. Этот компонент позволяет настроить сообщения, которые будут отправляться в Telegram.
   - В сообщение можно включить различную информацию, такую как количество строк с инцидентами, процентное соотношение проблемных строк к общему количеству записей и т.д.

3. **Автоматизация уведомлений**:
   - Уведомления могут быть настроены на регулярное выполнение через планировщик (доступно только на серверных версиях Loginom). Это позволяет автоматизировать процесс мониторинга и своевременно реагировать на возникающие проблемы.

4. **Интерактивные уведомления**:
   - В уведомления можно включать ссылки, которые ведут на соответствующие визуализаторы в Loginom. Это позволяет быстро перейти к проблемным местам и проанализировать данные без необходимости искать их вручную.
   - Ссылки можно настроить так, чтобы они вели на конкретные места пакета, что упрощает процесс анализа и устранения проблем.

5. **Гибкость использования**:
   - Механизм оповещений можно использовать не только для контроля качества данных, но и для других целей, таких как уведомления о завершении процессов, предупреждения о превышении пороговых значений и т.д.
   - Это делает систему оповещений универсальной и полезной для различных сценариев использования.

### Важность урока

Этот урок **особенно** важен!!! Так как наличие качественных и актуальных данных позволяет избежать искажений в результатах анализа и повышает доверие к аналитическим выводам.

--- 
## Материал
---

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/day3.png"/>
</div>

Прежде чем переходить к сборке слоя data-mart, разберем еще одну тему — мониторинг качества данных. В первом марафоне «Генеральная уборка данных» [![Loginom](https://img.shields.io/badge/Loginom-Visit%20Site-blue)](https://data-preprocessing.loginom.ru/)
 мы разбирали множество вариантов того, как недостаточно качественные данные могут исказить результаты анализа.

Вопрос в том, что у нас не всегда есть возможность постоянно контролировать отсутствие проблем и ошибок лично. И зачастую есть смысл узнавать о проблемах до того, как они всплывут в финальной аналитике.

<div>
<img src="https://loginom.ru/sites/default/files/global-files/textpage/2025/data-q.svg"/>
</div>

<h2 align="center">Качество данных</h2>

### Компонент для оповещений в Telegram

Откройте пакет прошлого занятия, зайдите в подмодель Sales_plan. Разместите там компонент «📁 Аудит качества данных» из Data_Control_Kit. Подайте ему на вход данные из импорта LGD-файла. [![Loginom](https://img.shields.io/badge/Loginom-Visit%20Site-blue)](https://help.loginom.ru/userguide/data-format/lgd-file.html)

Как вы можете видеть, «📁 Аудит качества данных» — это производный узел, а значит содержит внутри себя шаблон сценария, который мы будем настраивать.

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/derived-node-1.png"/>
</div>

<h2 align="center">Производный узел</h2>

Но для начала задайте на входном порту переменных название проверки, которую мы будем проводить.

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/check-3.png"/>
</div>

<h2 align="center">Проверка</h2>

Внутри нас ждет готовая схема.

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/ready-made-scheme-2.png"/>
</div>

<h2 align="center">Готовая схема</h2>

Сэмплинг позволяет отобрать определенное количество строк из разных частей таблицы, что особенно актуально при большом объеме данных, когда проверка всех записей становится нецелесообразной.

В калькуляторе «DQ проверки» задаются условия для контроля значений. Давайте зайдем в него.

Суть контроля данных заключается в создании логических полей, которые принимают значение true, если в данных выявлена проблема или присутствует фактор, который требуется отслеживать.

Название такого поля должно начинаться с префикса DQ_, за которым следует название проверки. Рассмотрим пример проверки значения в поле Amount_plan >= 30000.

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/checking-value-4.png"/>
</div>

<h2 align="center">Проверка значения в поле Amount_plan</h2>

Если требуется создать несколько проверок, достаточно клонировать поле проверки, задать ему новую метку и прописать новое условие.

Давайте создадим вторую проверку с условием Amount_plan <= 500.

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/second-check-5.png"/>
</div>

<h2 align="center">Вторая проверка</h2>

​​​​Представим, что таким образом мы будем контролировать наличие подозрительно больших и маленьких объемов в планах продаж.

Компонент «Отчет DQ» формирует сводную информацию на основе созданных полей. Сводка включает:

- количество строк с инцидентами;
- процентное соотношение этих строк к общему количеству записей.

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/component-6.png"/>
</div>

<h2 align="center">Компонент «Отчет DQ»</h2>

Одна из особенностей производных узлов — это возможность не только изменять настройки существующего сценария, но и добавлять в него собственные узлы.

Добавим узел Дубликаты и противоречия из стандартной библиотеки элементов между Калькулятором и «Отчетом DQ».

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/node-7.png"/>
</div>

<h2 align="center">Узел «Дубликаты и противоречия»</h2>

Давайте ради теста сделаем проверку дубликатов в поле Key_ERP_User_Sales_ID, указав его как входное поле.

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/test-8.png"/>
</div>

<h2 align="center">Тест</h2>  

Очень удобно, что в результате проверки создаются логические поля, принимающие значение true для дубликатов. Необходимо лишь переименовать метку на выходном порте поля, например, в «DQ_Дубли ИД менеджера», чтобы узел «Отчет DQ» включил его статистику в сводку.

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/verification-result-9.png"/>
</div>

<h2 align="center">Результат проверки</h2>  

Теперь статистика по дубликатам тоже формируется в сводке.

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/statistics-duplicates-10.png"/>
</div>

<h2 align="center">Статистика по дубликатам</h2>

Как узнать, что в данных есть проблема, не заходя в Loginom?

Так как в первый день мы посвятили время созданию и настройке Telegram-бота, активация последнего компонента в сценарии отправит сообщение в технический чат через бота. Это позволит оперативно получать уведомления о выявленных проблемах.

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/message-chat-11.png"/>
</div>

<h2 align="center">Сообщение в чат</h2>

Теперь, поставив выполнение компонента «📁 Аудит качества данных» на регулярное выполнение через планировщик (делается только на серверных версиях Loginom).

### Визуальный контроль проблем в данных
Когда пакетов станет много, вспомнить все контрольные точки может быть непросто. Однако это уже предусмотрено: компонент «Отчет DQ» содержит встроенные визуализаторы, которые позволяют быстро анализировать данные и выявленные проблемы.

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/component-12.png"/>
</div>

<h2 align="center">Компонент «Отчет DQ»</h2>

​​​​​Их можно использовать для быстрого просмотра и построения специфического отчета.

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/quick-view-13.png"/>
</div>

<h2 align="center">Быстрый просмотр</h2>

Кроме того, на серверных версиях Loginom есть возможность получить ссылку на любое место пакета.

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/link-14.png"/>
</div>

<h2 align="center">Ссылка на любое место пакета</h2>

​​​​​​Эту ссылку можно скопировать и вставить в переменную подмодели, отправляющей сообщение.

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/node.png"/>
</div>

<h2 align="center">Переменная подмодели</h2>

Вы можете прописать у себя тестовую ссылку, вроде https://ya.ru.

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/test-link-15.png"/>
</div>

<h2 align="center">Тестовая ссылка</h2>

В результате к вашему сообщению будет прикреплена ссылка, отправляющая вас на соответствующие визуализаторы.

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/notification-16.png"/>
</div>

<h2 align="center">Уведомление в Telegram</h2>

Как вы уже догадались, этот функционал можно использовать не только для оповещений о качестве данных.

Механизм работает на базе компонента «Отправить сообщение» из библиотеки API_Telegram, что позволяет создавать собственные системы оповещений и рассылок.

Напишите в чат, о чем бы вы могли или хотели оповещать ваших коллег таким образом?

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/api-tg-17.png"/>
</div>

<h2 align="center">API_Telegram</h2>

Как использовать контроль качества данных в сценарии
Вы можете использовать данный механизм:

1. На любом этапе ETL-процесса — для работы с сырыми данными, таблицами core-уровня, витринами и моделями.
2. Как неотъемлемую часть обработки потока данных — проверка может автоматически запускаться при каждом выполнении сценария.
3. В отдельных ветках — проверки данных можно расположить так, чтобы они запускались вручную или по расписанию через планировщик, независимо от основного потока данных.
4. Для создания отдельных пакетов контроля данных — для проверки больших объемов данных, чтобы не замедлять выполнение основного сценария.
5. С ограничением выборки — при работе с таблицами, содержащими миллионы или десятки миллионов строк, проверка всех записей может занимать слишком много времени. В таких случаях следует ограничить выборку, по которой выполняется проверка, например:
   - брать данные только за х последних дней.
   - использовать ограниченную выборку из случайных мест таблицы (сэмплирование).
   - с ограничением на этапе импорта в Loginom — вместо загрузки всех 50 млн записей и последующего отбора 500 тыс., лучше сразу запрашивать только нужные 500 тыс. записей.

## 3.Формирование data-mart уровня КХД. Зажигаем звезды
---

### Краткое изложение урока

Этот урок посвящен созданию уровня data-mart на базе сформированного core-уровня КХД. Основные моменты:

1. **Сравнение реляционной модели и «Звезды»**:
   - Реляционная модель с понятными связями между таблицами не всегда подходит для BI-систем, которые часто требуют структуры «Звезда» или «Снежинка».
   - Power BI — одна из немногих систем, способных работать с реляционной структурой без дополнительных преобразований.

2. **Структура «Звезда» и «Снежинка»**:
   - **«Звезда»**: центральная таблица показателей связана с таблицами-лучами через первичные ключи.
   - **«Снежинка»**: некоторые таблицы-лучи связаны с другими лучами, а не с центром, что усложняет структуру.

3. **Сборка центральной части «Звезды»**:
   - Объединение ключевых полей и полей показателей из разных таблиц с помощью операции Union.
   - Создание канонической даты для анализа данных на одной временной оси.
   - Добавление поля «Тип даты» для различения источников данных.

4. **Восстановление связей**:
   - Присоединение недостающих идентификаторов для корректной работы связей между таблицами.
   - Алгоритм присоединения ключевых полей для каждой таблицы модели.

5. **Автоматизация с помощью Data Monetization Pack**:
   - Использование компонентов для автоматического преобразования реляционной структуры в «Звезду».
   - Настройка параметров для формирования модели и управление составом витрины.

6. **Генерация модели данных**:
   - Создание витрины на основе метаданных и управление дополнительными полями.
   - Формирование календарных полей и шаблонов запросов для быстрой загрузки данных.

### Важность урока

Этот урок важен, так как он предоставляет практические инструменты и методы для преобразования данных в структуру, удобную для анализа в BI-системах. Это позволяет повысить эффективность аналитики и улучшить качество принимаемых решений. Автоматизация процесса с помощью Data Monetization Pack значительно упрощает задачу и снижает риск ошибок при преобразовании данных.

--- 
## Материал
---

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/day4.png"/>
</div>

На втором занятии мы сформировали core-уровень КХД. Напомню, там сейчас находится девять таблиц, которые связаны следующим образом.

<div>
<img src="https://loginom.ru/sites/default/files/global-files/textpage/2025/core-level.svg"/>
</div>

<h2 align="center">Сравнительная схема работы с КХД и без него</h2>

По сути, у нас здесь реляционная модель с понятными связями между таблицами. Так что же мешает нам загрузить данные в BI и визуализировать их?

Ответ прост: адекватно работать с реляционной структурой данных без дополнительных преобразований умеет только Power BI. Все остальные BI-системы (как российские, так и западные), когда речь идет об анализе сложных структур данных, ориентируются на то, что модель будет организована по топологии «Звезда» (и далеко не все поддерживают «Снежинку»).

Теоретически данные с уровня ядра можно загрузить в BI-систему и попытаться преобразовать их в «Звезду» с помощью встроенного ETL-функционала. Но в этом случае работать с этой моделью можно будет только внутри конкретной BI-системы, что не совсем удобно.

Поэтому имеет смысл не только организовать хранение очищенных данных (на базовом уровне), но и создать уровень с витринами/моделями (data mart). Именно этим мы и займемся.

<div>
<img src="https://loginom.ru/sites/default/files/global-files/textpage/2025/system-nfe-4.svg"/>
</div>

<h2 align="center">Поток данных</h2>

#### Разница между «Звездой» и «Снежинкой»
Основной принцип построения таких моделей заключается в том, что данные связываются не напрямую между таблицами, как в реляционной модели, а через центральную таблицу показателей и связей. Поля, размещенные в таблицах-лучах, представляют собой поля измерений.

В случае «Звезды» каждая таблица-луч соединяется с центром через свой первичный ключ.

<div>
<img src="https://loginom.ru/sites/default/files/global-files/textpage/2025/star.svg"/>
</div>

В случае «Снежинки» некоторые таблицы в лучах связываются не с центром, а с другим лучом, являясь своего рода расширяющими справочниками.

<div>
<img src="https://loginom.ru/sites/default/files/global-files/textpage/2025/snowflake.svg"/>
</div>

«Звезда» является более универсальной структурой, так как позволяет привязать данные из центра к любому лучу благодаря наличию соответствующего ключевого поля в центральной таблице.

В структуре «Снежинка» для корректной работы связей ключевые таблицы в центральной таблице должны соответствовать таблицам-лучам первого уровня. То есть нельзя просто добавить в центральную таблицу данные, которые связаны с таблицей «Менеджеры клиентов». Нужно преобразовать идентификаторы менеджеров клиентов в идентификаторы клиентов, и тогда связь сработает через таблицу «Клиенты».

#### Как собирается центральная часть «Звезды»
Базовый принцип: ключевые поля и поля показателей из таблиц, которые должны использоваться в модели, объединяются с помощью операции Union. Рассмотрим на примере двух таблиц: «Продажи» и «План продаж».

<div>
<img src="https://loginom.ru/sites/default/files/global-files/textpage/2025/sales-plan.svg"/>
</div>

Шаги показаны не с точки зрения очередности операций, а как основным этапы формирования. На уровне SQL-запроса, это конечно же, делается одним запросом.

#### Шаг 1 — через Union собираем поля связей и показателей в одну таблицу.

<div>
<img src="https://loginom.ru/sites/default/files/global-files/textpage/2025/sales-plan.svg"/>
</div>

#### Шаг 2 — создаем поле канонической даты: в нее записываются даты из полей, чьи данные должны анализироваться на одной временной оси.

<div>
<img src="https://loginom.ru/sites/default/files/global-files/textpage/2025/step-2.svg"/>
</div>

Шаг 3 — создаем поле «Тип даты», содержащее название поля, из которого должны брались даты для единой временной оси.

<div>
<img src="https://loginom.ru/sites/default/files/global-files/textpage/2025/step-3.svg"/>
</div>

Такая структура таблицы называются уровнями, слоями, этажами. Она позволяет создавать показатели с формулами вида:

- Выручка = sum_if(Сумма сделки, Тип даты = "Дата реализации")
- Потенциал открытых сделок = sum_if(Сумма сделки, Тип даты = "Дата открытия")
- План продаж = sum_if(Сумма плана, Тип даты = "Месяц плана")

Благодаря канонической временной оси результаты расчетов этих показателей можно выводить на одном графике времени (с группировкой от месяца и более крупными единицами времени), не переживая о том, что фактически это данные с разной гранулярностью по времени (продажи — по дням, план — по месяцам).

Теоретически, если для поля показателя используется только один вариант канонической даты (например, только сделки на дату реализации), то его можно не указывать в формулах. Однако при этом существует риск, что с добавлением нового уровня с другой канонической датой (например, сделки на дату создания) вам придется переделывать старые формулы, поскольку они теперь могут возвращать задвоенные данные, агрегированные по обоим типам дат.

#### Шаг 4 — создаем поле «Сущность», содержащее название таблицы, из которой взяты данные.

<div>
<img src="https://loginom.ru/sites/default/files/global-files/textpage/2025/step-4.svg"/>
</div>

По этому полю можно рассчитывать показатели с уровней без канонических дат (например, если это поле из справочника, которые не имеет своих дат).

Теперь к этой таблице через связи или через JOIN присоединяются поля-измерения в справочниках — и модель готова.

#### Какие бывают варианты «Звезды»
Модели топологии «Звезда» имеют 3 варианта.

<div>
<img src="https://loginom.ru/sites/default/files/global-files/textpage/2025/4.svg"/>
</div>

<h2 align="center">Модели топологии «Звезда»</h2>

Далеко не всегда уложить через UNION этажи центральной таблицы друг на друга достаточно для корректной работы модели.

Допустим, мы хотим преобразовать в «Звезду» следующую реляционную структуру. Как видно, при реляционном подходе нет никаких проблем с получением данных из таблицы

<div>
<img src="https://loginom.ru/sites/default/files/global-files/textpage/2025/relationship-contacts-table.svg"/>
</div>

Если сформировать центральную таблицу в виде этажей, мы получим картину, когда записи сделок не связаны с «Компаниями», потому что у сделок нет собственного CompanyID.

<div>
<img src="https://loginom.ru/sites/default/files/global-files/textpage/2025/without-companyID.svg"/>
</div>

Чтобы связи работали как задумано, нужно провести процесс «восстановления связей» — присоединить недостающие идентификаторы на соответствующий уровень таблицы.

<div>
<img src="https://loginom.ru/sites/default/files/global-files/textpage/2025/with-companyID.svg"/>
</div>

Делать это можно на этапе формирования core-уровня или при построении модели в data-mart.

Как понять, какие ключевые поля нужно присоединить на этаж? Для каждой таблицы модели нужно выполнить следующий алгоритм (разбираем для таблицы «Сделки»).

<div>
<img src="https://loginom.ru/sites/default/files/global-files/textpage/2025/relationship-contacts-table.svg"/>
</div>

1. Если в таблице 1 («Сделки») есть вторичный ключ («ContactID» в сделках), который является первичным ключом в таблице 2 («Контакты»).
2. То из таблицы 2, в которой это поле является первичным ключом («Контакты») нужно присоединить ключевые поля, отсутствующие в таблице 1 («CompanyID» присоединяются к таблице «Сделки»).
3. Для присоединенных ключей нужно повторить шаги 1 и 2, пока не исчерпается глубина связей для таблицы 1 (количество уровней связи равно n-1, где n — количество таблиц в модели);
4. Повторяем для остальных таблиц модели по мере увеличения глубины связей.

#### Проверьте себя, насколько вы поняли данный принцип. Ответьте на 2 вопроса:

1. Какой набор ключевых полей для уровня «Сделки» будет в итоговой центральной таблице?

Ответ:
«ИД Сделок», «ИД Контакта», «ИД компании», «ИД Категории», «ИД Пользователя», «ИД Деятельности», «ИД Классификатора».

2. Из какой таблицы в уровень «Сделки» будет добавлен «ИД Пользователя»?

Ответ:
Из таблицы «Контакты», т.к. она ближе по уровню вложенности.

<div>
<img src="https://loginom.ru/sites/default/files/global-files/textpage/2025/table-connection.svg"/>
</div>

#### Генерация модели данных с помощью Data Monetization Pack
Как вы наверняка поняли, преобразование реляционной структуры в «Звезду» — задача достаточно сложная и трудоемкая. Одной из ключевых сложностей является необходимость следить за целостностью связей и регулярно перепроверять ее при добавлении новых данных в модель. Это особенно важно, когда модель строится на основе десятков таблиц core-уровня произвольной структуры.

Тем не менее есть и положительный момент: реляционная структура любой сложности может быть преобразована в «Звезду» с помощью фиксированной последовательности шагов. Учитывая это, мы разработали компонент, который выполняет такие операции автоматически.

Для использования компонента откройте Loginom Community Edition и создайте новый пакет в расположении M3\Libs\Data Monetization Pack\Examples.

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/model-generation-1.png"/>
</div>

<h2 align="center">Генерация моделей</h2>

Перейдите в раздел «Ссылки» и добавьте ссылки на пакеты Clickhouse_Kit, Data_Preparation_Kit.

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/package-links-2.png"/>
</div>

<h2 align="center">Ссылки на пакеты Clickhouse_Kit и Data_Preparation_Kit</h2>

Добавьте в сценарий компонент «3. Структура хранилища» из Data_Preparation_Kit и активируйте его. Этот компонент показывает на первом выходе структуру прямых связей таблиц core-уровня КХД. А на втором — рекомендованные действия по процессу восстановления связей.

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/component_3.png"/>
</div>

<h2 align="center">Компонент «3. Структура хранилища»</h2>

Помните вот это место в схеме данных core-уровня? Чтобы можно было анализировать данные продаж в разрезе всех этих справочников, в них при сборе модели нужно будет присоединить недостающие идентификаторы из таблицы «Клиенты» и «Классификаторы».

<div>
<img src="https://loginom.ru/sites/default/files/global-files/textpage/2025/customers.svg"/>
</div>

В кросс-табличных связях показывается последовательность присоединений, наборы ключей и таблицы-источники ключей. Можно воспользоваться этой информацией и выполнить восстановление на уровне ETL. А можно ничего не делать, и тогда при генерации модели эти операции выполнятся автоматически (это наш путь сегодня).

Добавьте в сценарий компонент «3.1 Список таблиц (мета-справочник)». У него из порта выходит полная таблица метаданных: список полей в хранилище, их описание и различные характеристики.

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/CH.png"/>
</div>

<h2 align="center">Компонент «3.1 Список таблиц (мета-справочник)»</h2>

Это не только увлекательное чтиво, но и инструмент управления автоматизациями на уровне КХД. Из Clickhouse_Kit добавьте компонент «🔑 CH: Создать денорм. звезду».

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/stars_5.png"/>
</div>

<h2 align="center">Компонент «🔑 CH: Создать денорм. звезду»</h2>

Этот компонент используется для создания витрины на основе списка полей из метаданных по алгоритму формирования «Звезды», описанному выше.

Вы можете управлять составом витрины, фильтруя перечень входящих полей встроенным функционалом Loginom.

Зайдите в порт переменных, чтобы задать настройки формирования модели. Разберем их.

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/settings_6%20%281%29.png"/>
</div>

<h2 align="center">Настройки</h2>

**Домен** — это база данных, в которой размещается витрина. К названию базы данных добавляется префикс доменной базы, указанный в настройках файла ClickhouseKit.txt. В одном домене может находиться несколько витрин. Для аналитиков можно настроить доступ только к данным определенного домена, регулируя права учетной записи пользователя базы данных.

**Название ядра** — название таблицы в БД, в которой сформируется витрина. К названию также добавится префикс модели из настроек ClickhouseKit.txt.

**ORDER BY** — последовательность полей, по которым осуществляется сортировка (подробнее разбиралось в занятии №2). Если оставить поле пустым, набор полей будет определен автоматически: в него войдут все измерения витрины в порядке возрастания их гранулярности. На данный момент мы оставим это поле пустым.

**Включать в модель только уровни с мерами** — добавлять ли записи таблиц без полей показателей (чистые справочники) как этажи в центральную таблицу.

**Имена полей в нижнем регистре** — имена полей созданной витрины будут в нижнем регистре (важно для некоторых систем).

**Алиасы в нижнем регистре** — после завершения генерации будут сформированы шаблоны SELECT-запросов для загрузки данных из витрины. В самой витрине поля называются в соответствии с их названиями в Loginom. Один из шаблонов содержит вариант запроса с переименованием полей в псевдонимы на основе меток, например: SELECT «sls_gross_profit» AS «прд_валовая прибыль». Этот параметр определяет, будут ли метки отображаться в нижнем регистре или сохранят свое исходное написание.

**Публиковать** — по аналогии с выгрузкой в core-уровень, сначала витрина формируется во временной базе, а по завершению мгновенно переносится в указанные в настройках реквизиты. Если установить значение false, то формирование витрины будет происходить только на уровне временной базы без замещения рабочей витрины.

Ряд настроек может подаваться в табличном виде. Второй порт данных принимает набор дополнительных полей, которые будут созданы в модели после ее формирования с помощью SQL-формул. Основное применение — создание календарных полей. Для этой задачи есть компонент-шаблон, который уже содержит основные заготовки формул.

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/creating-calendar-fields-12.png"/>
</div>

<h2 align="center">Создание календарных полей</h2>

По умолчанию компонент формирует набор формул для канонической даты модели, но если на вход подать список полей из справочника метаданных, то для каждого поля с типом DateTime будет сформирован календарь.

С помощью фильтров до и после компонента «CH: Календарь модели» вы можете определить, какие именно поля календаря, для каких именно полей дат будут созданы в модели.

Шаблоны формул календаря лежат в файле: Марафон 3\Data Monetization Pack\Settings\ClickhouseKit_cal.txt. Значение $1 в шаблоне будет заменено на соответствующее поле даты при генерации.

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/calendaformula-templates_13.png"/>
</div>

<h2 align="center">Шаблоны формул календаря</h2>

**Не стоит использовать порт создания дополнительных полей как способ доделать что-то, что было упущено на этапе ETL. Если вам нужны новые поля измерений или показателей — рассчитывайте их в таблицах на core-уровне. Календарные поля — чуть ли не единственное исключение для этого случая.**

Выполните компонент. После выполнения вы получите структуру созданной таблицы, которая возвращается с помощью команды DESCRIBE TABLE. Это подтверждает успешное создание таблицы в базе данных.

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/component-execution_7.png"/>
</div>

<h2 align="center">Выполнение компонента</h2>

Второй порт данных содержит шаблоны формул показателей для разных BI-систем. Как их использовать и почему они такие, какие есть — разберем в следующем занятии.

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/indicator-templates_14.png"/>
</div>

<h2 align="center">Шаблоны показателей</h2>

В выходных переменных содержатся шаблоны запросов для быстрой подстановки в загрузку данных.

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/request-templates_8.png"/>
</div>

<h2 align="center">Шаблоны запросов</h2>

Добавьте ссылку на пакет Clickhouse Connections.lgp, который расположен в M3\Libs\Data Monetization Pack\Settings\Clickhouse Kit, чтобы у вас появилось настроенное подключение к Clickhouse.

Настройте импорт из БД, использовав шаблон запроса SELECT AS из генератора витрины.

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/import-database_9.png"/>
</div>

<h2 align="center">Импорт из БД</h2>

Результат:

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/result_10.png"/>
</div>

<h2 align="center">Результат</h2>

Используйте ваши навыки работы в Loginom, чтобы создать Визуализатор, на котором будет показан план и факт по выручке в разрезе товарных групп. Экспериментируйте с моделью на ваше усмотрение (Визуализатор Куб)!

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/visualizer_11.png"/>
</div>

<h2 align="center">Визуализатор</h2>

#### А что там происходит, внутри генератора моделей?
Используя список метаданных, переданных на вход, и логику, описанную в сегодняшнем занятии, Loginom формирует SQL-запрос, который выполняется на стороне ClickHouse. Это означает, что данные в Loginom во время генерации не загружаются — весь процесс выполняется за счет обращения к базовому уровню КХД.

Для диагностических целей, запрос сохраняется во временной базе, в таблице create_query (в демо-режиме не работает, поэтому можете скачать файл с этим запросом, чтобы понять, что он из себя представляет).

Практика:

Попробуйте использовать разные наборы полей при генерации моделей;
Попробуйте создать календари для разных полей дат.

## 1.Визуализация универсальной модели
---

### Краткое изложение


--- 
## Материал
---

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/day5.png"/>
</div>

И так, на прошлом занятии мы подготовили витрину данных, созданную по правилам сборки универсальной модели для бизнес-аналитики. Теперь пришла пора данные визуализировать.

Для этого мы будем использовать Yandex DataLens. У этой системы есть бесплатный тариф, она сразу работает в облаке и рассчитана на прямое чтение данных из Clickhouse — а это именно то, что нам нужно.

Откройте сервис по ссылке — потребуется учетная запись в Яндекс.

Откройте раздел «Подключения» в левой панели, и нажмите на кнопку «Создать подключение».

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/creating-connections_1.png"/>
</div>

<h2 align="center">Создание подключений</h2>

Выберите тип подключения Clickhouse.

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/connection-type_2.png"/>
</div>

<h2 align="center">Тип подключения</h2>

Укажите ручной вариант подключения и заполните реквизиты в соответствии с вашими данными. Обязательно проверьте подключение через кнопку слева внизу — должна загореться зеленая галочка.

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/manual-option.png"/>
</div>

<h2 align="center">Ручной вариант подключения</h2>

Теперь нужно создать датасет на основе нашего подключения.

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/creating-dataset_4.png"/>
</div>

<h2 align="center">Создание датасета</h2>

Задаем в качестве подключения нашу базу, находим в списке таблицу и перетаскиваем в рабочую область. На этом все — процесс сборки модели данных завершен! Все поля уже связаны внутри, остается только прописать формулы.

<div>
<img src="https://loginom.ru/sites/default/files/styles/image792/public/global-images/textpage/2025/database-connection_5.png"/>
</div>

<h2 align="center">Подключение БД</h2>

#### Основные правила создания формул показателей
Для работы с моделями совмещающие данные нескольких таблиц показателей, формируется поле канонической даты canonical_date (единая временная ось), а также справочное поле date_type, которое указывает, из какого исходного поля взяты значения для него.

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/table-1.png"/>
</div>

Таким образом, используя поле canonical_date (и производные календарные поля от него), можно агрегировать на одной временной оси показатели, которые изначально находились в разных таблицах (как продажи и план продаж на картинке выше).

На этапе формирования core-уровня хранилища для одной таблицы возможно указать несколько канонических дат (частый пример из CRM-систем: дата создания и дата закрытия у сделки). Таким образом, можно данные одной сущности аллоцировать на один из нескольких типов данных при агрегации.

Пример: две канонические даты в таблице сделок формируют следующую структуру в модели:

<div>
<img src="https://loginom.ru/sites/default/files/global-images/textpage/2025/table-2.png"/>
</div>

Чтобы агрегации корректно работали на моделях такой структуры, в их условиях нужно прописывать, по какому типу даты они должны агрегировать. Т.е. для примера выше мы должны будем создать показатели:

 - Сумма сделок на дату открытия — SUM_IF([Deal_sum], [date_type] = 'dl_Create_date')
 - Сумма сделок на дату закрытия — SUM_IF([Deal_sum], [date_type] = 'dl_Close_date')

Рекомендуется указывать в формулах агрегаций тип даты в явном виде, даже если он находится в единственном экземпляре для конкретной сущности. Потому что если в модели у сущности появится вторая каноническая дата, то простые агрегации вида SUM([Deal_sum]) будут отдавать задвоенные результаты, распределенные сразу на все типы дат.

Пренебрегать указанием типа дат имеет смысл только для тех сущностей, для которых вы уверены на 100% в том, что несколько канонических дат в них не будет.

